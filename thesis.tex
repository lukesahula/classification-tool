\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath} 
\usepackage[utf8]{inputenc}
\usepackage[czech]{babel}
\usepackage{url}
\usepackage{xcolor}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{url}
\usepackage{hyperref}
\usepackage{pgfplots}
\hypersetup{
  colorlinks = true,
  citecolor = gray,
}
\usepackage{algpseudocode, algorithm}

\urlstyle{same}
\bibliographystyle{IEEEtran}

\addto\captionsczech{
  \renewcommand{\contentsname}{Table of contents}%
  \renewcommand{\figurename}{Figure}%
  \renewcommand{\tablename}{Table}%
}
\title{Handling Missing Values in Decision Forests in the Encrypted Network Traffic}
\date{2018-05-06}
\author{Lukáš Sahula}

\begin{document}
  \maketitle
  \newpage
  \section*{Acknowledgement}
    {\color{red}TODO}
  \newpage
  \section*{Abstract}
    {\color{red}TODO}
    \\~\\
    {\bf Keywords:} malware, classification, random forests, supervised learning, missing values, imputation 
    \\~\\
    {\color{red}TODO}
    \\~\\
    {\bf Klíčová slova:} malware, klasifikace, náhodné lesy, učení s učitelem, chybějící hodnoty, imputace 
  \newpage
  \tableofcontents
  \newpage

  \section*{Introduction}
  \addcontentsline{toc}{section}{Introduction}
    Machine learning and cybersecurity are two highly significant subjects of today's tech industry. With machine learning being utilized in various fields at an increasing rate, it is no wonder that it has found its uses even in the field of cybersecurity. This thesis touches both of these subjects in the context of automated {\it malware classification}, with the main focus being dealing with {\it missing values} using {\it random forests}.
    \\~\\
    In order for a classification algorithm to work, it has to be trained on some set of data. In this case the datasets are encrypted network traffic and as such they contain a moderate amount of {\it missing data}. Most classification algorithms are not designed with an implicit way of dealing with missing values and thus they have to be handled before or during the classification process.
    \\~\\
    Over the past two decades\cite{otfi}\cite{lwd}\cite{mia}, a moderate amount of methods for dealing with missing data has been introduced, but none has yet shown superior results that would put it above the others. Plenty of methods are somewhat situational and work only with specific cases of missing data. Thus it is not clear which method to choose at which time. This thesis studies these methods along with analysing the network traffic datasets in order to find out the most efficient one.
    \\~\\
    The dataset used in this thesis contains real and huge data spanning across more than a hundred of enterprises. As such, one of the concerns in choosing the methods for handling missing data are its computational speed and memory demands.
    \\~\\
    The thesis consists of several sections. At the beginning, there is a brief introduction to malware classification, followed by an explanation of the classification algorithm often connected to the missing data problem - the {\it random forest classifier}. The next chapter concerns itself with missing data mechanisms and gives a summary of known algorithms for missing data {\it imputation}. The chapter after that provides an analysis of the network datasets. The last section is dedicated to the conducted experiments and results of this thesis.
  \newpage
  \section{Malware and classification}
      This chapter serves both as a general introduction to the term {\it malware} and as an introduction to {\it machine learning}, specifically to one of its subsets, {\it classification}. After explaining both terms, there is a short section dedicated to connecting the two terms.
    \subsection{Malware}
      {\it Malware} is an abbreviated form of the term malicious software. It is often used when referring to viruses, spyware, ransomware, and other software designed to cause harm to a computer, server, network or a mobile device.\cite{malware}
      \\~\\
      Viruses are computer programs with the goal of spreading from one file to another across one or multiple devices through a network undetected and without consent of the user. A common misinterpretation of viruses is that they are programs designed to delete or move data, however, this damage is often a side effect. The definition of a virus is that it spreads itself.\cite{malware}
      \\~\\
      Spyware is a form of computer program that runs on the infected computer and tracks its user's habits to form a pattern that can be then used for advertisements or sent to the spyware's creator.\cite{malware}
      \\~\\
      Ransomware is a potentialy very dangerous software that threatens to delete, block or publish the target's data if the programs authors requirements are not met.\cite{ransomware}
      \\~\\
      When malware spreads through the network, or when it communicates with their command and control computers, it leaves a detectable trace in the network traffic. What more, these traces can often be found much sooner (ranging from weeks to months) than researchers are able to capture a sample of the invading malware.\cite{network} This only emphasizes the importance of utilizing machine learning in the context of malware detection, manual systems and blacklists are not enough today.
      \\~\\
      According to the five years long study by Georgia Institute of Technology, the endpoints with which specific malware communicates do not change in long time periods (years). This means that once a node in the network suspicious of being infected is found, it is possible to look for traffic going in and out, which in turn can help with identifying other infected devices.\cite{network}
    \subsection{Classification}
      In statistics and machine learning, classification is the process of assigning a specific category, or a {\it class}, to which a new object or an observation belongs.\cite{elements} This assignment, or {\it prediction}, is done after processing a set of previously categorized data, often called the {\it training dataset}.\cite{elements} This is done by a classification algorithm, known as a {\it classifier}.\cite{elements} This classifier learns from a set of data, for example emails labeled as spam or non-spam, in order to predict the category of new incoming emails. That way an email client can move unwanted spam to the spam folder and keep the relevant mail in the inbox. Another is predicting whether a patient's tumor is benign or malign, based on the hospital records with data from other patients. Or, as will be the case in this thesis, whether an instance of network traffic is the related to (and which) malware, based on data gained from network proxy logs.
      \\~\\
      Within the terminology of machine learning, classification is in the category of {\it supervised learning}.\cite{mlintro} Supervised learning means that the classifier learns from the training dataset with labeled data. This dataset is often processed into the form of a matrix $\mathbf{X}$ in order to make the work with it more convenient. The rows in this matrix contain the observed attributes, called {\it features}, of each individual object. The rows are sometimes called {\it feature vectors}, labeled $\mathbf{x} = \left( x_1, x_2, x_3, ..., x_i \right)$ where $i$ denotes the number of features. Feature vector sometimes refers to the column, containing all values of a single feature across all objects. However, in this thesis, feature vectors refer to the rows. These features are quantifiable and can be categorical (smoker or non-smoker), integer-valued (age of the patient in years), or real-valued (patient's body temperature). Since the learning is supervised, the dataset also includes a column with the class labels of each observed object labeled $\mathbf{Y}$.
      \\~\\
      When the classifier is trained, it can predict the classes of new objects based on their features. The mathematical function implemented by the classifier maps input feature vectors $\mathbf{x}$ of a matrix $\mathbf{X}$ to a class $\mathbf{c}$ from the set of classes $\mathbf{C}$. The performance of the classifier is usually tested on another set of data called the {\it testing dataset}. That is done to avoid {\it overfitting} - "the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably".\cite{oxford} According to this definition, an overfitted classifier could be able to predict the classes of the objects in its training dataset accurately, yet completely miss the classes of new objects. The opposite of overfitting is {\it underfitting}, which can also be avoided by measuring the classifier's performance on the testing dataset. In machine learning, overfitting and underfitting is sometimes also called "overtraining" and "undertraining" respectively.
      \\~\\
      Classification does not always have to be binary, meaning the data can be of more than two categories. This is called {\it multiclass classification}. Returning to the example of e-mail classification, some classifiers could be trained to predict whether an incoming e-mail belongs to {\it work}, {\it school}, or {\it personal} categories.
      \\~\\
      There are also other forms of classification, like the {\it multilabel classification}. In multilabel classification, multiple classes can be assigned to an observation. This makes sense in cases where the categories are not mutually exclusive. However, this thesis will be focusing only on the problem of multiclass classification.
    \subsection{Malware classification}
      As was mentioned in the malware section of this chapter, malware can often be detected very early by monitoring the incoming and outgoing network traffic of the observed device. However, this is not something that every computer user can do, and even then it can prove to be a relatively tedious task. In this day and age there can be thousands of individual network requests performed by a computer and going through them manually seems almost impossible. As such, it would be much more convenient to take all the data and feed it to a machine learning algorithm to do the work instead.
      \\~\\
      This thesis is based on the work done on labeled network datasets containing observations of more than two hundred different classes of malware. The goal is not only to be able to predict whether a computer is infected with malware, but also to decide which class of malware infection it is. It is, therefore, a case of multiclass classification.
      
  \newpage
  \section{Random forest classifier}
    {\it Random forests} or {\it random forest classifiers} are the central focus of this thesis because they are most suited for the malware classification problem. They can handle {\it multiclass classification}, evaluate data relatively fast and they are not heavily affected by {\it imbalanced datasets}.\cite{brabec} They run efficiently on large amounts of data and give estimates of what feature variables are important for classification.\cite{breiman}
    \\~\\
    This chapter explains in detail how random forests work and how they can be implemented. As the name implies, random forest is an ensemble of learning algorithms called {\it decision trees}. In order to understand random forests, decision trees are the first thing that needs to be explained.
    \subsection{Decision trees}
      \subsubsection{Overview}
        {\it Decision tree classifier} is a classification algorithm named because of its structure resembling a tree known from graph theory. Another variation of a decision tree is the regression tree, which only differs in that the predicted outcome is not a class but a real value. One of the most significant publications about decision trees is {\it Classification and Regression Trees} by L. Breiman. It introduces the {\it CART algorithm} for growing both classification and regression trees.\cite{cart} Other algorithms for growing decision trees also exist, for example the {\it ID3 algorithm} or the {\it C4.5 algorithm}.
        \\~\\
        Prediction using a decision tree can be interpreted as a set of questions asking about the attributes of the observed object. For example, predicting whether a picture is taken outdoors or indoors could work as follows:
        \begin{itemize}
          \item Is the top of the picture blue?
          \begin{itemize}
            \item If yes, the picture was taken outdoors.
            \item If no, is the bottom of the picture green?
            \begin{itemize}
              \item If yes, the picture was taken outdoors.
              \item If no, the picture was taken indoors.
            \end{itemize}
          \end{itemize}
        \end{itemize}
        As can be seen in the example, a grown tree is built from a root node, decision nodes and terminal nodes, also called {\it leaves}. All leaves are associated with the resulting prediction class. Each decision node contains information about its split, which consists of the selected feature and its value. When an observed object is analysed to predict its class, the value of the feature in question is compared with the value stored at the decision node. Depending on the result of this comparison, the object is then sent to the left child or the right child of the node. When the object reaches a leaf node, it is given the class that is associated with it. This explanation is visualised in figure [\ref{figure:decision_tree}].
        \begin{figure}
          \centering
          \includegraphics[scale=0.7]{thesis_res/decision_tree.png}
          \caption{Picture of a decision tree with the training data used for its growth.\cite{digstaining}}
          \label{figure:decision_tree}
        \end{figure}
      \subsubsection{Strengths}
        In comparison with other classification algorithms, decision trees have various strengths and advantages, like:
        \begin{itemize}
        \item They require little data preparation, for example data normalisation is not needed like in other techniques.\cite{isl}
        \item They work with both numerical and categorical data.\cite{isl}
        \item They perform well with large datasets within reasonable time.\cite{breiman}
        \item They are easily interpretable by most people after a brief explanation. The graphical representation of the tree is also very easy to follow and the decision making process is similar to how people generally make decisions in real life.\cite{isl}
        \end{itemize}
      \subsubsection{Weaknesses}
        However, decision trees are not perfect as classifiers go because they also have some weaknesses, for example:
        \begin{itemize}
        \item They are not as accurate as other approaches.\cite{isl}
        \item They are not very robust, a small change in training data can lead to a big change in the decision tree, leading to different results.\cite{isl}
        \item They are prone to {\it overfitting}.\cite{isl}
        \end{itemize}
        All these disadvantages are going to be addresed again in the random forests part of this chapter.
      \subsubsection{Growing a decision tree}
        Training a decision tree classifier is done by recursively partitioning the input data according to the best calculated split. The split specifies the feature and value at which the data is partitioned into two parts. Once the data is partitioned, a new node $t_i$ is created and the process continues at its children, see algorithm [\ref{alg:grow_tree}]. If it is for some reasons impossible or not needed to split the input dataset, a leaf node is created and the recursion does not continue.\cite{brabec}
        \\~\\
        To expand on the notation, the nodes are numbered in breadth-first order, starting with the root node $t_0$. Since all the trees encountered in this thesis are binary, the left child of node $t_i$ can be denoted as $t_i^L$ and the right child can be denoted as $t_i^R$. Since the dataset is being recursively partitioned, the subset of samples belonging to node $t_i$ can be denoted as $S_i$. The subset of samples belonging to the left child of node $t_i$ is denoted as $S_i^L$ and the one belonging to the right child as $S_i^R$. It follows that $S_i = S_i^L \cup S_i^R$ and that $S_i^L \cap S_i^R = \theta$ for every inner node.\cite{brabec}
        \\~\\
        \begin{algorithm}
          \caption{This algorithm shows how the decision tree classifier is grown recursively. It uses the dataset $S_i$ as the input and returns the root node $t_0$ of the tree.\cite{brabec}}
          \label{alg:grow_tree}
          \begin{algorithmic}[1] % The number tells where the line numbering should start
            \Function{GrowTree}{$S_i$}
              \If{$ShouldCreateLeafNode(S_i)$}
                \State \Return $CreateLeafNode(S_i)$
              \EndIf
              \State $\theta \gets FindBestSplitParameters(S_i)$
              \State $S_i^L, S_i^R \gets SplitDataset(S_i, \theta)$
              \State $t_i^L \gets GrowTree(S_i^L)$
              \State $t_i^R \gets GrowTree(S_i^R)$
              \State \Return $t_i$
            \EndFunction
          \end{algorithmic}
        \end{algorithm}
      \subsubsection{When to create a leaf node}
        Multiple conditions are checked before the dataset $\mathbf{S_i}$ is split, depending upon which the split may not even take place and a leaf node gets created. The conditions may differ among different implementations of the tree. The ones used for the purposes of this thesis are the following:
        \begin{itemize}
          \item Check the number of unique classes in dataset $\mathbf{S_i}$. If all the samples are of the same class, there is no need to split further and a leaf node can be created.\cite{brabec}
          \item Check if it is possible to split the dataset $\mathbf{S_i}$ at least once. It is possible, that all the feature vectors in $\mathbf{S_i}$ are the same, but their classes differ. If that is the case, a leaf node is created.\cite{brabec}
          \item One of the hyperparameters of a decision tree is usually the minimum number of samples required to split the dataset ($minSamplesToSplit$). If $|\mathbf{S_i}| < minSamplesToSplit$ then a leaf node is created. This parameter is often set low in random forests because it allows the trees to grow deeper.\cite{brabec}
        \end{itemize}
        Other implementations also include the $maxDepth$ hyperparameter, limiting the maximum depth of a tree, or the change in {\it impurity measure} (information gain) as the stopping condition during node splitting.\cite{brabec}
      \subsubsection{How to create a leaf node}
        The leaf node contains the subset of sampled associated with it. Upon arriving in a leaf node during prediction, the most common choice is to return the class with maximum frequency in the samples. Different implementations can return the whole normalized histogram of classes in the samples, where each value represents the probability of the object belonging to the given class.
      \subsubsection{Finding the best split}
        \label{sec:split}
        The best split out of all possible splits of the dataset is defined as the one that provides the largest {\it information gain}. Information gain corresponds to the decrease in impurity in the node's children. {\it Impurity measure} of node's subset $\mathbf{S_i}$ is a function $i(\mathbf{S_i}) \in \mathbb{R}$. The purer the data (the lower the impurity measure), the more confidence can be given in the node's prediction. Therefore, the split $\theta_i$ that reduces the impurity in the node's children the most, is also the best split. To find it, all possible splits have to evaluated. The decrease in impurity is defined as follows:
        \begin{equation}
          \Delta i(\mathbf{S_i}, \theta_i) = i(\mathbf{S_i}) - \frac{|\mathbf{S_i^L}|i(\mathbf{S_i^L}) + |\mathbf{S_i^R}|i(\mathbf{S_i^R})}{|\mathbf{S_i}|}
        \end{equation}
        There are several impurity measures, most common of which are {\it Gini impurity} and {\it entropy}. Studies show that there is not a significant difference between the two and therefore can be used interchangebly.\cite{gini} Only entropy is used for the purposes of this thesis. Entropy is defined as:
        \begin{equation}
          H(\mathbf{X}) = - \sum_{c \in \mathbf{C}}^{n} p(c)\log{p(c)}
        \end{equation}
        Where $\mathbf{C}$ is the set of classes and $p(c)$ is the frequency of class $c$ in the node's subset $\mathbf{S_i}$.\cite{brabec} If the class is not in the subset at all, it does not add to the sum anything either. The entropy is at its maximum if the classes in the subset $\mathbf{S_i}$ are equally distributed. If all the objects belong to the same class, the entropy equals to zero. As was stated earlier, information gain corresponds to the decrease in impurity $\Delta i(\mathbf{S_i}, \theta_i)$.\cite{brabec}
        \\~\\
        To select the best split, all splits along every dimension have to be evaluated.\cite{brabec}. This can be optimised, as is shown in algorithm [\ref{alg:best_split}]. The data points are grouped together to reduce duplicities. This means that out of 50 objects having the same coordinate in the evaluated dimension, we only remember the class counts of those 50 objects and the coordinate itself. Further work in finding the split is done only with the class counts.\cite{brabec}
        \begin{algorithm}
          \caption{This algorithm shows how to find the split with the highest information gain.\cite{brabec}}
          \label{alg:best_split}
          \begin{algorithmic}[1] % The number tells where the line numbering should start
            \Function{FindBestSplitParameters}{$S_i$}
              \State $maxGain_\Delta \gets -\infty$
              \For{$all dimensions$}
                \State $counts^L \gets (0, 0, ..., 0)$
                \State $counts^R \gets GetClassCounts(S_i)$
                \State $coordinates \gets$ find unique coordinates in dimension
                \State $Sort(coordinates)$
                \For{$point \in coordinates$}
                  \State $counts^L \gets$ add class counts present on point
                  \State $counts^R \gets$ remove class counts present on point
                  \State $currentGain_\Delta = ComputeGain_\Delta(counts^L, counts^R)$
                  \If{$currentGain_\Delta > maxGain_\Delta$}
                    \State $maxGain_\Delta \gets currentGain_\Delta$
                    \State $\theta_i \gets$ create split parameters from point and neighbor
                  \EndIf
                \EndFor
              \EndFor
              \State \Return $\theta_i$
            \EndFunction
            \Function{$ComputeGain_\Delta$}{$counts^L, counts^R$}
              \State $size^L \gets Sum(counts^L)$
              \State $size^R \gets Sum(counts^R)$
              \State \Return $- \frac{size^L \times H(counts^L) + size^R \times H(counts^R)}{size^L + size^R}$
            \EndFunction
          \end{algorithmic}
        \end{algorithm}
        Since many features can have only a very limited set of values, this optimisation can give a significant performance boost to the algorithm.\cite{brabec} By sorting the coordinates, the class counts can be added to $counts^L$ and removed from $counts^R$ in an organised way. That is done by iterating over all of the sorted coordinates throughout the dimension. This approach also makes the time of each entropy calculation constant because it is dependent only on the number of classes, instead of the number of objects.\cite{brabec}
        \\~\\
        After finding the best possible split $\theta_i$, the dataset $\mathbf{S_i}$ is then partitioned into $\mathbf{S_i^L}$ and $\mathbf{S_i^R}$ by a threshold set by the splits feature and value. Considering the number of dimensions and classes are constant, the worst case time complexity of this algorithm is $\mathcal{O}(|\mathbf{S_i}|\log{\mathbf{S_i}})$. The worst case is the one where there are no data points with duplicate coordinates and the sorting of the coordinates will equal to sorting the whole dataset. In practice, however, it can be expected that the performance will be better, but still highly dependant on the given data.\cite{brabec}
    \subsection{Random forests}
      Since single decision forests tend to overfit to the training data\cite{isl}, are not making very accurate predictions, and tend to change significantly with small alterations to the datasets, they do not look very good compared to other approaches. However, an {\it ensemble method} called random forests deals with these issues. It groups multiple randomized decision trees together and builds a much stronger classifier. There are two categories of ensemble methods, {\bf boosting methods} and {\bf averaging methods}. Boosting methods create the weak classifiers sequentially and every classifier is an improvement of the previous one.\cite{brabec} Boosting methods are not in the scope of this thesis, so only averaging methods are going to be discussed.
      \\~\\
      Averaging methods build the classifiers in parallel and average their predictions. This averaging decreases the potentially high variance of the weak classifiers. Random forests belong to this category.\cite{brabec} Depending on the implementation, the predictions are aggregated either by {\it majority voting} or by {\it soft voting}. Majority voting chooses the most frequent class among the predictions of all the trees, while soft voting averages the class probability results from each tree.\cite{ensemble} For random forests to bring an improvement over decision trees, each tree in the forest has to be different. This is where the "random" in the name comes from. Randomness is injected into each tree in different ways, depending on the implementation. It is usually done by introducing randomness to the node splitting process, or by selecting a random subsample from the training dataset.\cite{brabec} {\it Breiman forests}\cite{breiman}, by far the most commonly used random forest algorithm, combine {\it bootstrap aggregating} and {\it random feature subsets} at each node to inject randomness into the forest. This is done in this thesis as well.
      \subsubsection{Bootstrap aggregating}
        Bootstrap aggregating, also called {\it bagging}, was first introduced in\cite{bagging}. For a forest of size $m$ and training dataset of size $n$, bagging assigns a dataset of size $n'$ to each of the $m$ trees. These datasets are created by selecting random samples with replacement from the training dataset. This means that some objects are not present in the sampled dataset and some objects might be repeated. The trees are then trained on their corresponding sample. The size of the bootstrapped datasets is usually parametrized in the forest, but the most common choice is to select $n'$ = $n$.
      \subsubsection{Random feature subsets}
        Random feature subsets inject randomness directly into the growing process of the decision tree. When looking for the best possible split, a standard decision tree looks across all dimensions of the training data.[\ref{alg:best_split}] However, trees in Breiman forests only use a random subset of the dimensions, sampled for each split node individually. The number of dimensions selected by this sampling is another hyperparameter of the random forest, denoted in this thesis as $maxFeatures$. The value of this parameter affects the correlation between individual trees.\cite{brabec} The least corellation would be achieved by setting $maxFeatures = 1$. Selecting $maxFeatures = numberOfDimensions$ removes all randomness from the individual trees' splitting process, leaving only the randomness induced by bagging. That, however, is usually not enough. More so when the training dataset contains a big amount of objects. In this particular case, the forest would not bring many advantages over a decision tree. An empirically tested rule of thumb says that $splitFeaturesCount = \sqrt{numberOfDimensions}$ is a reasonable default for classification tasks.\cite{ert} Curiously enough, the default for regression classes seems to be equal to the number of dimensions.\cite{ert}
  \newpage
  \section{Handling missing values}
    In statistics, {\it missing values} or {\it missing data} mark the absence of value in a feature variable of an observed sample. Missing data within a dataset make it impossible for conventional machine learning algorithms to properly learn from it. Many of these algorithms require complete data and do not have an implicit way of handling missing values. In order for the statistical analysis to work, the missing data have to be somehow dealt with beforehand.\cite{otfi}
    \\~\\
    There are various ways of handling missing data, some as simple as dropping the samples containing any missing values altogether, leaving only those samples with all data present.\cite{lwd} This however, cannot be done when there are missing data somewhere in most of the observations. Another simple method, the strawman imputation\cite{otfi} would be replacing missing data with the mean or median of all the non-missing values of the feature in question. The process of replacing the missing data is called {\it imputation}. Only the imputation methods related to random forests are examined in this thesis because of the nature of the analysed data. Also, most of the interesting imputation methods already are in this subset.
    \\~\\
    More sophisticated methods of imputation also exist and they are discussed more thoroughly in this thesis. Some methods work better with smaller datasets, or with datasets with low missingness ratio. The amount of correlation between feature variables can also be important to some methods and not that important to others.\cite{otfi} Some imputation algorithms are very slow and using them in experiments with big datasets can get seriously time-consuming. In other words, choosing a relevant method of imputation for a given dataset is not a simple task and it can prove useful to do an analysis of the data as well as an analysis of the imputation algorithm itself beforehand.
    \subsection{Related work}
      Various works comparing different imputation methods were published in the last years. Multiple imputation methods meant for decision trees are well explained in\cite{mia}.
      \\~\\
      Another work worth mentioning that focuses on random forests and missing data imputation is\cite{rsf}. It introduces a new type of random forests along with the {\it adaptive tree imputation} method. However, this method can be used with regular random forests as well.
      \\~\\
      Perhaps the most recent work on this topic is\cite{otfi} which is a continuation of the previous work by one of the same authors. In this paper, multiple approaches to imputation using random forests are discussed in detail and tested on multiple datasets with different attributes. A comparison of their imputation accuracy is made as well. The datasets used in this work are both real and synthetic. However, the missingness is induced manually\cite{otfi} and thus the results of the experiments can be biased.
    \\~\\
    \subsection{Missing data mechanisms}
      \label{sec:missmech}
      Missing data are usually divided into three categories, depending on the characteristics of their missingness. These categories are also called missing data mechanisms.\cite{lwd} It is important to understand these mechanisms in order to make an educated assumption about the dataset in question. If an assumption is made that the values are missing completely at random while they are missing systematically, an analysis based on this assumption may be biased.
      \begin{itemize}
      \item If the samples with missing data are a random subset of all observed samples, they have a similar distribution. There is no relationship between whether a value is missing and any other value in the data set, be it missing or observed. These values are {\it missing completely at random} (MCAR).\cite{lwd} When a dataset has missing values that are MCAR, it can still be analysed with unbiased results, provided that there are enough observed samples. However, data that are truly MCAR are not encountered often.
      \item When the missing data is somehow dependent or related to other non-missing values in the observation, the data is {\it missing at random} (MAR).\cite{lwd} There is no definitive way of distinguishing between MCAR and MAR data. The assumption of it being one or the other is only as good as the knowledge of the data and the field of the one proposing the assumption. A good example of MAR data would be that males are less likely to fill a depression survey, although it does not have any connection to their level of depression.
      \item The last missing data mechanism is {\it missing not at random} (MNAR). It means that there is a relationship between value of the variable that is missing and the reason why it is missing in the first place. For example a male suffering from a strong depression can decide not to fill a depression survey because of said depression.
      \end{itemize}
    \subsection{Selected imputation algorithms}
      This section contains a list of imputation algorithms that were taken in consideration when deciding which ones to implement.
      \subsubsection{Baseline imputation}
        \label{sec:baseline}
        This method will be used throughout the thesis as the baseline reference value. It replaces the missing data with a constant value outside of the features' interval before growing the tree. Thus, when an object is evaluated and the relevant feature is missing, the object is always sent either to the left or to the right, based on the value that was chosen at the start.
      \subsubsection{Strawman imputation}
        \label{sec:strawman}
        {\it Strawman imputation}\cite{otfi} is a simple method for handling missing values. It works very fast compared to other methods and its implementation is very simple. The missing values are imputed before the forest is grown by calculating the median of non-missing values in the feature column. An altered variation of strawman imputation uses the mean instead of the median.
      \subsubsection{On-the-fly-imputation}
        \label{sec:otfi}
        {\it Adaptive tree imputation}\cite{rsf} later named as {\it On-the-fly-imputation}\cite{otfi} (OTFI) is a method of imputing missing data at the time of growing the tree. That is also where its name comes from. It draws a random value from the non-missing in-bag dataset within the current node to impute the missing values. The algorithm works as follows: 
        \begin{enumerate}
        \item First, the best split is calculated as usual, using only non-missing data.
        \item After finding the best split, random values from the non-missing in-bag data within the current node are used to impute the missing values.
        \item Once the data are imputed, the node is split into the left and right children nodes and the imputed data are reset back to missing.
        \end{enumerate}
        The values the algorithm uses to impute the missing data are stored within the decision node while growing the tree along with the respective frequencies in which they occur within the node's dataset. These are then used again to impute the missing values in the testing dataset at the time of prediction to decide whether the observation belongs to the left or to the right child. After being sent to the child node, the imputed data is reset to missing again and the process repeats.
        \\~\\
        The implementation of OTFI algorithm used for experiments in this thesis had to be improved in order to increase imputation speed and reduce the memory used. Instead of storing the non-missing values and their frequencies, only the probability of the node going left or right is saved at the node. That way, when a new object with a missing value at the relevant feature is examined, the missing value is not imputed at all.
      \subsubsection{Missingness incorporated in attributes}
        \label{sec:mia}
        {\it Missingness incorporated in attributes}\cite{mia} (MIA) works similarly to OTFI in that it also imputes missing data during the forest growing process. It searches for the best split in three different approaches to the missing data. Let {\it X} be a numeric feature used to split a node and {\it s} a possible split value of {\it X}. Over all split values, the method looks at the following:
        \begin{itemize}
        \item Split A: \{ $X \leq s$ or $X =$ missing \} versus \{ $X > s$ \}.
        \item Split B: \{ $X \leq s$ \} versus \{ $X > s$ or $X =$ missing \}.
        \item Split C: \{ $X =$ missing \} versus \{ $X =$ not missing \}.
        \end{itemize}
        After finding the best split in each approach separately, the algorithm then chooses which one of them provides a better information gain and then it splits the dataset accordingly. As in the OTFI method, the decision node here remembers which split type it had assigned in order to decide how the testing data should be propagated during prediction.
        \\~\\
        This algorithm is in practice an extension of the method that was used as a baseline. The baseline in fact does the same thing as split A in here. It follows that this method takes approximately three times more time than the baseline.
      \subsubsection{Other algorithms}
        Other notable imputation algorithms were also considered, such as {\it surrogate splits}\cite{splits} or {\it missForest}\cite{otfi}. However, surrogate splits perform less reliably as the amount of missing data increases\cite{splits} and are slow when used with larger datasets.\cite{rsf} The low computational speed is a problem for missForest as well, making it "100's of times" slower than methods like OTFI or MIA.\cite{otfi}
  \section{Network dataset}
    This chapter centers on the network datasets used for malware classification. The following subsections provide a description of the dataset as well as an analysis done to gather more information on the correlation among pairs of features. The last part of this chapter tries to answer why most of the studied algorithms do not provide satisfying results.
    \subsection{Dataset description}
      The data comes from network traffic of more than a hundred large enterprises taken on five days of early 2017. The dataset contains roughly 600 million of individual records. Out of these, only 4 million are labeled as positive, making the dataset heavily imbalanced. Imbalanced dataset means that "at least one of the classes constitutes only a very small minority of the data."\cite{imbalanced} In this case, each of the positive classes are a minority while the negative class is an overwhelming (more than 99\%) majority.
      \\~\\
      Each object in the dataset consists of 55 columns. The first column represents the class to which the object belongs. The next 3 columns contain metadata information of the object and are not used for training and prediction. Those are the timestamp, user and host and they are used in aggregated evaluation of the classifier, described later in chapter [\ref{sec:experiments}]. The remaining 51 columns are the features. Because of the sensitivity of the data, the features themselves are not represented by their names and their values are encrypted, turning them into integers and real numbers. However, some examples of the features could be the number of bytes transmitted or received, duration of the network event, direction of the packet, the amount of incoming and outgoing packets, the IP address of the destination, etc.
      \\~\\
      An important thing that needs to be taken into consideration when working with a dataset like this is the fact that the samples are taken from various days. This could potentially mean that a classifier can perform better on data taken from the same day or from the day after, but worse with data from the next month, or even a year. The samples analysed in this thesis are from days week apart in January, with one day from the beginning of March. In this small scale, however, no significant differences in performance that could be attributed to this were found.
    \subsection{Analysis}
        This subchapter examines the dataset and attempts to analyse the nature of missingness of its missing data. The first step of the analysis was to look at the features and see how often and when are they missing. After that, a {\it correlation matrix} was constructed to see the correlation among pairs of the features. Another correlation matrix was constructed from an altered dataset where each value was replaced by 1 and each missing value was replaced by 0 to see the how correlated the features' missingness is. A matrix of conditional probabilities was also created to provide information on how often is one feature not missing when the other one is missing.
      \subsubsection{Missingness}
        In the whole dataset, there are only 4 features that are not missing from any of the objects. 12 features, on the other hand, are missing in more than 90\% of the time. The average missingness throughout the dataset is roughly 51\%. Some features are closely tied together and when one of them is missing, the others are missing as well. Therefore the assumption is that some of the features are not missing at random.[\ref{sec:missmech}] Other features, however, do not fulfill this assumption and could possibly be used when others are missing if they are somehow correlated.
      \subsubsection{Correlation matrix}
        The second step in this analysis was to figure out how are the features correlated to each other. For that, the {\it Pearson correlation coefficient} was used, defined as: \cite{correlation}
        \begin{equation}
          \rho_{X, Y} = \frac{cov(X, Y)}{\sigma_X \sigma_Y}
        \end{equation}
        In this equation $X$ and $Y$ are features and the result is the amount of correlation between them. Correlation is bound between -1 and 1: \cite{correlation}
        \begin{equation}
          -1 \leq \rho_{X, Y} \leq 1
        \end{equation}
        The closer the correlation is to 1, the stronger the positive linear dependence between the features is and vice versa, the closer the correlation is to -1, the stronger the negative linear dependence between the features is.
        \\~\\
        Computing this correlation for each existing pair of all the features in the dataset we get a 51 x 51 symmetric matrix with the number 1 on the main diagonal. Every element in this matrix symbolizes the correlation between the feature represented by the element's row, and the feature represented by the element's column. The matrix is symmetric because the pair of features (X, Y) has the same correlation as the pair (Y, X). Furthermore, the main diagonal is full of 1s because a feature is completely correlated with itself. However, in some cases it is impossible to compute the correlation, and that is when the feature is always missing. In this thesis, its correlation value is replaced by 0. With this matrix, it is possible to create a heatmap in order to provide a more visual representation[\ref{figure:correlation_matrix}].
      \subsubsection{Missingness correlation matrix}
      \subsubsection{Conditional probabilities matrix}
    \subsection{Why most of the algorithms is not relevant}
      {\color{red}TODO}
  \newpage
  \section{Experiments}
    \label{sec:experiments}
    This chapter focuses on the experiments done with network data regarding the performance of several imputation methods. The first part describes the measures that were evaluated and why they were used. The second part looks at the results of the experiments and their comparison. All of the experiments were done on the same data, that is the network traffic logged on three days of January 2017, each of them a week after the previous one, was used as the training dataset, whereas the testing dataset was a single day of March of the same year. Each experiment was evaluated in three different ways - {\it unaggregated}, {\it aggregated}, and {\it aggregated relaxed}. Unaggregated means that the performance is evaluated regularly. Aggregated (by one of the metadata column, for example a user) evaluation is done in order to remove duplicities (multiple records marking the same user as a suspect of malware infection) and removes negative records if there is at least one positive one. This makes sense because if a user is infected, the network traffic he produces can be either regular or point to some malware infection. This aggregation removes the "unimportant, clean" data. Going a level further, the aggregated relaxed evaluation clears the differences between different classes of malware and reduces the classification problem into the binary case - infected, or not infected.
    \subsection{Evaluation metrics}
      There is a plenty of measures that can be used to determine the classifier's performance. The most commonly observed is perhaps the {\it classification error}\cite{brabec}. The problem with classification error is that it does not say much when the dataset is imbalanced, as is the situation with the network data. For example if we had a dataset where there are 10,000 samples and only 1 of them belonged to a positive class, the classifier could classify all samples as negative and it would achieve a classification error of 0.01 \%. In this project we are interested in two different measures, {\it precision} and {\it recall}. In order to compute them, we also need the {\it confusion matrix}.
      \subsubsection{Confusion matrix}
        In binary classification, we can label the two classes as positive and negative. {\it Confusion matrix}\cite{confusion} divides the classification results into the following categories:
        \begin{description}
        \item [TP:]   True positives. The number of positive objects the classifier labeled as positive.
        \item [FP:]   False positives. The number of negative objects the classifier labeled as positive.
        \item [TN:]   True negatives. The number of negative objects the classifier labeled as negative.
        \item [FN:]   False negatives. The number of positive objects the classifier labeled as negative.
        \end{description}
        These categories are then used to compute other, more interesting measures.
        \\~\\
        In multiclass classification, the {\it confusion matrix} gets more complicated. Consider a dataset with 3 different classes A, B and C. If the classifier takes a sample that belongs to class A and labels it as B, then it counts as FN for class A, but as FP for class B.
        \\~\\
        This gets even more confusing with the network datasets. The samples are either negative and thus belonging to one specific class, or positive, which means one of multiple positive classes. For that reason, samples belonging to the negative classes that are classified as negative do not count as TP. Similarly, when a positive sample is labeled as positive but a different class, it should not be counted as FN, rather as TPish. As for TNs, it does not make much sense to distinguish them from TPs, since they basically are TPs for the negative class. This is a way of reducing the multiclass problem to a binary problem. In the context of this project, a {\it one-vs-all} confusion matrix, which evaluates one class at a time, is computed. Furthermore, TNs do not get counted and the TPs of the negative class are optionally filtered out in the process.
      \subsubsection{Precision}
        {\it Precision}, or {\it positive predictive value}\cite{confusion} is defined as:
        \begin{equation}
          Precision = \frac{TP}{TP + FP}
        \end{equation}
        Precision is the fraction of positive objects among all objects that the classifier labeled as positive. It can also be interpreted as the probability that a positively labeled object is truly positive. A precision score of 1.0 means that every object labeled as positive by the classifier is truly positive. It does not say, however, anything about the classifier's ability to recognize all truly positive instances. In multiclass environment, precision of every class varies because it is computed separately.
      \subsubsection{Recall}
        {\it Recall}, or {\it sensitivity}\cite{confusion} is defined as:
        \begin{equation}
          Recall = \frac{TP}{TP + FN}
        \end{equation}
        Recall is the fraction of positive objects that the classifier labeled as positive among all truly positive objects. It can be interpreted as the probability that a truly positive object is labeled as positive. A recall score of 1.0 means that every truly positive object is labeled as positive by the classifier. This could be achieved simply by labeling all objects as positive, recall does not say anything about the number of false positives. Recall of every class varies among all classes in multiclass environment.
    \subsection{Results}
      This subsection presents the results of the individual experiments. The main focus is the average prediction precision, recall and also the number of classes that were predicted with precision above specific thresholds. The following tables will always show all three evaluation approaches (unaggregated, aggregated, relaxed) for every tested method. The tested methods are the baseline[\ref{sec:baseline}] method, the Strawman[\ref{sec:strawman}] imputation method both with mean and median values, the OTFI[\ref{sec:otfi}] method and the MIA[\ref{sec:mia}] method. The two variations of the Strawman method are further referred to simply as mean and median.
      \subsubsection{Average overall precision}
        The first metric to look at is the overall precision. It is the precision in the form that was declared in at the beginning of this chapter. Looking at figure [\ref{figure:overall_prec}], it can be seen that the OTFI method does not bring any improvement compared to the baseline and its results are actually even weaker.
        \begin{figure}
          \centering
          \caption{Average overall precision of the tested methods.}
          \label{figure:overall_prec}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6046999288156045) (Mean, 0) (Median, 0) (Otfi, 0.2286460222752374) (Mia, 0.6417303494717684)
              };
              \addplot coordinates {
                (Baseline, 0.5129532415312706) (Mean, 0) (Median, 0) (Otfi, 0.2010929298915794) (Mia, 0.5563848885786536)
              };
              \addplot coordinates {
                (Baseline, 0.5300755806248237) (Mean, 0) (Median, 0) (Otfi, 0.20144624029104627) (Mia, 0.571316912278676)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
      \subsubsection{Average overall precision without the negative class}
        In some situations, it is relevant to only see the precision of prediction of the positive classes, discarding the negative one. Especially in this case, where the dataset is heavily imbalanced and the amount of negative objects drastically exceeds the amount of positive ones. This precision is shown in figure [\ref{figure:overall_prec_no_legit}].
        \begin{figure}
          \centering
          \caption{Average overall precision of the tested methods, excluding the negative class.}
          \label{figure:overall_prec_no_legit}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6100512556192825) (Mean, 0) (Median, 0) (Otfi, 0.23076311507408218) (Mia, 0.6474093791131115)
              };
              \addplot coordinates {
                (Baseline, 0.5174926507483615) (Mean, 0) (Median, 0) (Otfi, 0.20295490146464956) (Mia, 0.561308648654571)
              };
              \addplot coordinates {
                (Baseline, 0.5347665149666363) (Mean, 0) (Median, 0) (Otfi, 0.2033114832567041) (Mia, 0.5763728141572483)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
      \subsubsection{Average overall precision without nans}
        In some cases, one or more classes do not have any precision statistics because the classifier never predicted this specific class to any of the objects. They are represented as {\it not a number}, shortened as {\it nan} values. When computing average precision, these classes add to the amount of classes by default. Figure [\ref{figure:overall_prec_no_nan}] shows the average overall precision while skipping them altogether.
        \\~\\
        It shows a big increase in OTFI method precision, which gives some insight into why it has such low scores in the other categories. The split statistics computed when growing a tree[\ref{sec:split}] are done only on non-missing data.[\ref{sec:otfi}] As such, the OTFI method drops all the missing data and because a big portion of the network data itself is missing, there is not much left for the tree to grow properly.
        \begin{figure}
          \centering
          \caption{Average overall precision of the tested methods, excluding classes that were never predicted.}
          \label{figure:overall_prec_no_nan}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6825325929205833) (Mean, 0) (Median, 0) (Otfi, 0.9585544780000337) (Mia, 0.7315725983978161)
              };
              \addplot coordinates {
                (Baseline, 0.5789769260848006) (Mean, 0) (Median, 0) (Otfi, 0.8430434368531597) (Mia, 0.6342787729796651)
              };
              \addplot coordinates {
                (Baseline, 0.5983031306062367) (Mean, 0) (Median, 0) (Otfi, 0.8445246227586171) (Mia, 0.6513012799976906)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
      \subsubsection{Average overall precision without nans and the negative class}
        This evaluation approach is a combination of the previous two.[\ref{figure:overall_prec_no_nan_no_negative}] The OTFI method has a hundred percent precision overall precision in this case. This means that the classifier predicted correctly all the positive classes it recognized. This, however, does not say much, because the number of positive classes it did recognized is much smaller than the other methods.
        \begin{figure}
          \centering
          \caption{Average overall precision of the tested methods, excluding classes that were never predicted and the negative class.}
          \label{figure:overall_prec_no_nan_no_negative}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6893579188497893) (Mean, 0) (Median, 0) (Otfi, 0.9968966571200351) (Mia, 0.7389622206038545)
              };
              \addplot coordinates {
                (Baseline, 0.5847666953456485) (Mean, 0) (Median, 0) (Otfi, 0.8767651743272862) (Mia, 0.640685629272389)
              };
              \addplot coordinates {
                (Baseline, 0.6042861619122991) (Mean, 0) (Median, 0) (Otfi, 0.8783056076689617) (Mia, 0.6578800808057481)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
      \subsubsection{Number of classes with precision above a certain threshold}
        Average overall precision works as a metric to give a solid first view of the classifier's performance. However, it does not say much about the balance of the classifier. If the testing dataset had for example 90 objects of class 1 and the remaining 10 objects were of 10 different classes, the average overall precision could be 0.9. Sadly, that does not say if the classifier recognized all the classes, or only the one with 90 samples.
        \\~\\
        Because of that, it is good to look at the number of classes that were predicted with precision above some chosen threshold. The thresholds chosen for this experiment are 0.5[\ref{figure:prec_above_0.5}], 0.8[\ref{figure:prec_above_0.8}], 0.9[\ref{figure:prec_above_0.9}], 0.95[\ref{figure:prec_above_0.95}] and 1.0[\ref{figure:prec_above_1.0}]
        \begin{figure}
          \centering
          \caption{Number of classes with precision above 0.5}
          \label{figure:prec_above_0.5}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6893579188497893) (Mean, 0) (Median, 0) (Otfi, 0.9968966571200351) (Mia, 0.7389622206038545)
              };
              \addplot coordinates {
                (Baseline, 0.5847666953456485) (Mean, 0) (Median, 0) (Otfi, 0.8767651743272862) (Mia, 0.640685629272389)
              };
              \addplot coordinates {
                (Baseline, 0.6042861619122991) (Mean, 0) (Median, 0) (Otfi, 0.8783056076689617) (Mia, 0.6578800808057481)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
        \begin{figure}
          \centering
          \caption{Number of classes with precision above 0.8}
          \label{figure:prec_above_0.8}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6893579188497893) (Mean, 0) (Median, 0) (Otfi, 0.9968966571200351) (Mia, 0.7389622206038545)
              };
              \addplot coordinates {
                (Baseline, 0.5847666953456485) (Mean, 0) (Median, 0) (Otfi, 0.8767651743272862) (Mia, 0.640685629272389)
              };
              \addplot coordinates {
                (Baseline, 0.6042861619122991) (Mean, 0) (Median, 0) (Otfi, 0.8783056076689617) (Mia, 0.6578800808057481)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
        \begin{figure}
          \centering
          \caption{Number of classes with precision above 0.9}
          \label{figure:prec_above_0.9}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6893579188497893) (Mean, 0) (Median, 0) (Otfi, 0.9968966571200351) (Mia, 0.7389622206038545)
              };
              \addplot coordinates {
                (Baseline, 0.5847666953456485) (Mean, 0) (Median, 0) (Otfi, 0.8767651743272862) (Mia, 0.640685629272389)
              };
              \addplot coordinates {
                (Baseline, 0.6042861619122991) (Mean, 0) (Median, 0) (Otfi, 0.8783056076689617) (Mia, 0.6578800808057481)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
        \begin{figure}
          \centering
          \caption{Number of classes with precision above 0.95}
          \label{figure:prec_above_0.95}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6893579188497893) (Mean, 0) (Median, 0) (Otfi, 0.9968966571200351) (Mia, 0.7389622206038545)
              };
              \addplot coordinates {
                (Baseline, 0.5847666953456485) (Mean, 0) (Median, 0) (Otfi, 0.8767651743272862) (Mia, 0.640685629272389)
              };
              \addplot coordinates {
                (Baseline, 0.6042861619122991) (Mean, 0) (Median, 0) (Otfi, 0.8783056076689617) (Mia, 0.6578800808057481)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
        \begin{figure}
          \centering
          \caption{Number of classes with precision of 1.0}
          \label{figure:prec_above_1.0}
          \begin{tikzpicture}
            \begin{axis}[
                ybar,
                width=12cm,
                height=10cm,
                symbolic x coords={Baseline, Mean, Median, Otfi, Mia},
                xtick=data,
                legend style={at={(0.5, -0.2)},
                  anchor=north,legend columns=-1},
                x tick label style={
                  rotate=90,
                  anchor=east,
                },
            ]
              \addplot coordinates {
                (Baseline, 0.6893579188497893) (Mean, 0) (Median, 0) (Otfi, 0.9968966571200351) (Mia, 0.7389622206038545)
              };
              \addplot coordinates {
                (Baseline, 0.5847666953456485) (Mean, 0) (Median, 0) (Otfi, 0.8767651743272862) (Mia, 0.640685629272389)
              };
              \addplot coordinates {
                (Baseline, 0.6042861619122991) (Mean, 0) (Median, 0) (Otfi, 0.8783056076689617) (Mia, 0.6578800808057481)
              };
              \legend{Unaggregated, Aggregated, Relaxed}
            \end{axis}
          \end{tikzpicture}
        \end{figure}
  \newpage
  \section{Future work}
  \newpage
  \section*{Conclusion}
  \addcontentsline{toc}{section}{Conclusion}
    {\color{red}TODO}
  \newpage
  \bibliography{thesis}
\end{document}

