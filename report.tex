\documentclass{article}

\usepackage{mathtools} 
\usepackage[utf8]{inputenc}
\usepackage[czech]{babel}
\usepackage[]{algorithm2e}

\addto\captionsczech{
  \renewcommand{\contentsname}
    {Table of contents}
}

\title{Toolkit for machine learning experiments with decision forests on network data}
\date{2017-12-31}
\author{Lukáš Sahula}

\begin{document}
  \maketitle
  \newpage
  \section*{Abstract}
    The aim of the project is to implement a toolkit for malware classification that will be used in the followup Bachelor's thesis for machine learning experiments upon datasets with missing values. Given the size and form of the data taken from network proxy logs, the toolkit has to be able to properly load potentially large datasets, train a classifier and evaluate its results. This thesis describes the form and relevance of the network datasets, gives an introduction to malware classification and specifies the measures that are evaluated by the toolkit. A large portion of the thesis focuses on the toolkit implementation and the problems surfacing during the process.
    \\~\\
    Záměrem tohoto projektu je implementovat knihovnu pro klasifikaci malwaru, která bude využita v následující bakalářské práci pro experimenty na datasetech s chybějícími hodnotami ve strojovém učení. Vzhledem k velikosti a formě dat získaných ze síťových proxy logů musí knihovna umět správně nahrávat potenciálně velká data, natrénovat klasifikátor a vyhodnotit jeho výsledky. Práce popisuje formu a relevanci těchto síťových dat, podává stručný úvod do klasifikace malwaru a specifikuje vyhodnocované veličiny. Velká část práce se zabývá samotnou implementací jednotlivých modulů knihovny a problémy, které se v průběhu objevily.
  \newpage
  \tableofcontents
  \newpage

  \section*{Introduction}
  \addcontentsline{toc}{section}{Introduction}
    This thesis focuses on the implementation of a machine learning toolkit for malware classification. The purpose of this toolkit is to be able to load variously sized datasets previously extracted from network proxy logs, preprocess these datasets, use them to fit a classifier and evaluate the classifier's performance. Because the toolkit has three arguably different jobs to do - loading, classification and evaluation of the datasets - it is separated into three independent modules. Each module is designed to be working as a standalone piece of program and to leave the possibility open of applying it to a different problem than malware classification with minor changes.
    \\~\\
    The thesis consists of several sections. At the beginning, there is a brief introduction to machine learning and malware classification, followed by a short explanation of the network datasets. The next chapter gives a summary of the measures evaluated in the classification results. The chapter after that delves into the implementation process and breaks down the three modules of the toolkit one by one. The last section is dedicated to the random forest classifier algorithm that will have to be implemented from scratch in order to provide more possibilities for future experimentation.
  \newpage
  \section{Malware classification}
    {\it Classification} is the process of assigning a label (or a class) from a set of categories to a new observation based on its {\it features}. An example could be a classification algorithm that predicts whether a patient's tumor is benign or malign based on its size and the age of the patient. In this specific case, an observation is a single record from the network dataset. The algorithms doing the classification are called {\it classifiers}. Classification is a form of {\it supervised learning}. This means that the classifier learns from a previously classified set of records called the {\it training dataset}. The classifier learns from this training dataset and is then able to predict the class of a new observation with some level of accuracy. To test the performance of the classifier, usually a different set of labeled records called the {\it testing dataset} is used. It is important to separate the testing data from the training data to see if the classifier can perform adequately on previously unseen observations. Since there are multiple classes of malware in these network datasets, the classification process is called {\it multiclass classification}. The classifier does not simply state if the observation is malware or non-malware, it also states which class of malware the observation belongs to. From the set of classifier algorithms, the {\it random forest classifier} is the one that suits this malware classification problem the most. It handles multiclass classification, evaluates the data relatively fast and is not heavily affected by {\it imbalanced datasets}.
  \section{Network data}
    The datasets extracted from network proxy logs used in this project are in the form of zipped csv files, where each file contains proxy logs of a single user. They are divided into {\it negative} samples and {\it positive} samples. Negative samples form the majority of the data and they are those that were previously labeled as benign (non-malware). Positive samples were previously classified as some specific class of malware. When there are significant differences in counts of the classes of the dataset, which is the case here, it is called an {\it imbalanced dataset}. The samples are further diversified by the day they were taken, which provides the opportunity to observe the differences in performance of a classifier trained on data from one day and tested on data from a week later compared to the performance of the same classifier tested on data from a month or a year later. The {\it feature vector} of these samples contains 55 features but a significant amount of the records is missing some of its feature values. To deal with this missing values problem is the goal of the bachelor's thesis. The missing values are simply replaced by a constant for this project's purposes, since it does not affect the random forest classifier strongly. Apart from the label and the feature vector, each record contains a few metadata entries, like the timestamp of the record, that are used further in the evaluation process.
  \newpage
  \section{Evaluation measures}
    There is a plenty of measures that can be observed in classifier's performance. The most commonly observed is perhaps the {\it classification error}. The problem with classification error is that it does not say much when the dataset is imbalanced, as is the situation with the network data. For example if we had a dataset where there are 10,000 samples and only 1 of them belonged to a positive class, the classifier could classify all samples as negative and it would achieve a classification error of 0.01 \%. In this project we are interested in two different measures, {\it precision} and {\it recall}. In order to compute them, we also need the {\it confusion matrix}.
    \subsection{Confusion matrix}
      In binary classification, we can label the two classes as positive and negative. Confusion matrix divides the classification results into the following categories:
      \begin{description}
      \item [TP:] True positives. The number of positive observations the classifier labeled as positive.
      \item [FP:] False positives. The number of negative observations the classifier labeled as positive.
      \item [TN:] True negatives. The number of negative observations the classifier labeled as negative.
      \item [FN:] False negatives. The number of positive observations the classifier labeled as negative.
      \end{description}
      These categories are then used to compute other, more interesting measures.
      \\~\\
      In multiclass classification, the confusion matrix gets more complicated. Consider a dataset with 3 different classes A, B and C. If the classifier takes a sample that belongs to class A and labels it as B, then it counts as FN for class A, but as FP for class B. With the network datasets it gets even more confusing. The samples are either negative and thus belonging to one specific class, or positive, which means one of multiple positive classes. For that reason, samples belonging to the negative classes that are classified as negative do not count as TP. Similarly, when a positive sample is labeled as positive but a different class, it should not be counted as FN, rather as TPish.
    \newpage
    \subsection{Precision}
      {\it Precision}, or {\it positive predictive value} is defined as:
      \begin{displaymath}
        Precision = \frac{TP}{TP + FP}
      \end{displaymath}
      Precision is the fraction of positive objects among all objects that the classifier labeled as positive. It can also be interpreted as the probability that a positively labeled object is truly positive. A precision score of 1.0 means that every object labeled as positive by the classifier is truly positive. It does not say, however, anything about the classifier's ability to recognize all truly positive instances. In multiclass environment, precision of every class varies because it is computed separately.
    \subsection{Recall}
      {\it Recall}, or {\it sensitivity} is defined as:
      \begin{displaymath}
        Recall = \frac{TP}{TP + FN}
      \end{displaymath}
      Recall is the fraction of positive objects that the classifier labeled as positive among all truly positive objects. It can be interpreted as the probability that a truly positive object is labeled as positive. A recall score of 1.0 means that every truly positive object is labeled as positive by the classifier. This could be achieved simply by labeling all objects as positive, recall does not say anything about the number of false positives. Recall of every class varies among all classes in multiclass environment.
  \newpage
  \section{Toolkit implementation}
      This chapter takes a long, hard look at the toolkit's implementation process and the three modules it consists of. The programming language chosen for this task is Python because of its advantages and convenience when it comes to fast prototyping and machine learning.
    \subsection{Loading tool}
      This module contains mainly functions to load training and testing data. Apart from that, there are also some data preprocessing functions like data quantization, that did not fit into any of the other modules. A function to load a csv with classifications for the purpose of evaluation is also in this module.
      \subsubsection{Load training data}
        In order to train a classifier on the network datasets, the first issue is to load the csv files containing the training data into a data structure. That is a straightforward task, but working with relatively large datasets comes with a problem regarding memory. It is practically impossible to load the data from the whole day into memory even on high-end machines. As was mentioned earlier, the network datasets are imbalanced, the amount of negative samples is multiple times larger than the amount of positives. This fact provides an option to take a random sample of the negatives and train the classifier with this sample. Since each csv file is specific for a different user, it is not a safe practice to take a random sample of the files. To avoid this potential information loss, the better way is to go through each file, take a sample from it and concatenate it to the data structure.
      \subsubsection{Load testing data}
        The testing dataset, however, should not be sampled at all. Luckily, the classifier does not require all the testing data at once. It is possible to send it to the classifier part by part, which can be done in Python simply by turning the function into a {\it generator function} and instead of returning the data it {\it yields} it. That way it is possible to iterate through this function in a for loop and call the prediction function of the classifier on each iteration.
      \subsubsection{Quantize data}
        The feature values of every feature vector in the network datasets span across large intervals and to increase the classifier's performance, it is helpful to {\it quantize} the values into a set number of bins. This is done by computing a set number of quantiles and then replacing all the values with the quantile value. After some testing, the classifier had the best results with 16 bins.
      \subsubsection{Load classifications}
        After the classifier is done with the classification, the classification tool saves all its predictions into a csv file with columns containing the true labels and the predicted labels and, optionally, the metadata values mentioned in the previous chapter. This produces a large csv file that has to be processed by the evaluation tool part by part. Like the LoadTestingData function, this function is also a generator function that reads the classifications in parts of a specific number of lines.
    \newpage
    \subsection{Classification tool}
      This module works as a superstructure to the classifier that is passed as a parameter to the constructor. It encapsulates the classifiers functions and simplifies the classification process into two functions, one to fit the classifier and the other to use it to predict the classes of the testing data and save the results into a file.
      \subsubsection{Train classifier}
        All this function does is that it passes the data in the correct form into the classifiers fit function. Since most relevant classifiers implement this function, it should work no matter what classifier is given to the classification tool.
      \subsubsection{Save predictions}
        This function reads the testing data chunk by chunk and calls the classifiers predict function upon every chunk. After predicting each chunk, it then saves the predicted and the true class and the metadata into a new csv file.
    \newpage
    \subsection{Evaluation tool}
      The purpose of this module is to create the confusion matrix and compute precision and recall for every class from the classifier's output.
      \subsubsection{Compute stats}
      \subsubsection{Compute aggregated stats}
      \subsubsection{Compute precision}
      \subsubsection{Compute recall}
      \subsubsection{Get average precision}
      \subsubsection{Get average recall}
      \subsubsection{Get stats count}
    \newpage
  \section{Future work}
    \subsection{Random forests}
    \newpage
  \section*{Conclusion}
  \addcontentsline{toc}{section}{Conclusion}
\end{document}
