\documentclass{article}

\usepackage{mathtools} 
\usepackage[utf8]{inputenc}
\usepackage[czech]{babel}
\usepackage{listings}
\lstset{language=Python}

\addto\captionsczech{
  \renewcommand{\contentsname}
    {Table of contents}
}

\title{Toolkit for machine learning experiments with decision forests on network data}
\date{2017-12-31}
\author{Lukáš Sahula}

\begin{document}
  \maketitle
  \newpage
  \section*{Abstract}
    The aim of the project is to implement a toolkit for malware classification that will be used in the followup Bachelor's thesis for machine learning experiments upon datasets with missing values. Given the size and form of the data taken from network proxy logs, the toolkit has to be able to properly load potentially large datasets, train a classifier and evaluate its results. This thesis describes the form and relevance of the network datasets, gives an introduction to malware classification and specifies the measures that are evaluated by the toolkit. A large portion of the thesis focuses on the toolkit implementation and the problems surfacing during the process.
    \\~\\
    Záměrem tohoto projektu je implementovat knihovnu pro klasifikaci malwaru, která bude využita v následující bakalářské práci pro experimenty na datasetech s chybějícími hodnotami ve strojovém učení. Vzhledem k velikosti a formě dat získaných ze síťových proxy logů musí knihovna umět správně nahrávat potenciálně velká data, natrénovat klasifikátor a vyhodnotit jeho výsledky. Práce popisuje formu a relevanci těchto síťových dat, podává stručný úvod do klasifikace malwaru a specifikuje vyhodnocované veličiny. Velká část práce se zabývá samotnou implementací jednotlivých modulů knihovny a problémy, které se v průběhu objevily.
  \newpage
  \tableofcontents
  \newpage

  \section*{Introduction}
  \addcontentsline{toc}{section}{Introduction}
    This thesis focuses on the implementation of a machine learning toolkit for malware classification. The purpose of this toolkit is to be able to load variously sized datasets previously extracted from network proxy logs, preprocess these datasets, use them to fit a classifier and evaluate the classifier's performance. Because the toolkit has three arguably different jobs to do - loading, classification and evaluation of the datasets - it is separated into three independent modules. Each module is designed to be working as a standalone piece of program and to leave the possibility open of applying it to a different problem than malware classification with minor changes.
    \\~\\
    The thesis consists of several sections. At the beginning, there is a brief introduction to machine learning and malware classification, followed by a short explanation of the network datasets. The next chapter gives a summary of the measures evaluated in the classification results. The chapter after that delves into the implementation process and breaks down the three modules of the toolkit one by one. The last section is dedicated to the random forest classifier algorithm that will have to be implemented from scratch in order to provide more possibilities for future experimentation.
  \newpage
  \section{Malware classification}
    {\it Classification} is the process of assigning a label (or a class) from a set of categories to a new observation based on its {\it features}. An example could be a classification algorithm that predicts whether a patient's tumor is benign or malign based on its size and the age of the patient. In this specific case, an observation is a single record from the network dataset. The algorithms doing the classification are called {\it classifiers}. Classification is a form of {\it supervised learning}. This means that the classifier learns from a previously classified set of records called the {\it training dataset}. The classifier learns from this training dataset and is then able to predict the class of a new observation with some level of accuracy. To test the performance of the classifier, usually a different set of labeled records called the {\it testing dataset} is used. It is important to separate the testing data from the training data to see if the classifier can perform adequately on previously unseen observations. Since there are multiple classes of malware in these network datasets, the classification process is called {\it multiclass classification}. The classifier does not simply state if the observation is malware or non-malware, it also states which class of malware the observation belongs to. From the set of classifier algorithms, the {\it random forest classifier} is the one that suits this malware classification problem the most. It handles multiclass classification, evaluates the data relatively fast and is not heavily affected by {\it imbalanced datasets}.
  \section{Network data}
    The datasets extracted from network proxy logs used in this project are in the form of zipped csv files. They are divided into {\it negative} samples and {\it positive} samples. Negative samples form the majority of the data and they are those that were previously labeled as benign (non-malware). Positive samples were previously classified as some specific class of malware. When there are significant differences in counts of the classes of the dataset, which is the case here, it is called an {\it imbalanced dataset}. The samples are further diversified by the day they were taken, which provides the opportunity to observe the differences in performance of a classifier trained on data from one day and tested on data from a week later compared to the performance of the same classifier tested on data from a month or a year later. The {\it feature vector} of these samples contains 55 features but a significant amount of the records is missing some of its feature values. To deal with this missing values problem is the goal of the bachelor's thesis. The missing values are simply replaced by a constant for this project's purposes, since it does not affect the random forest classifier strongly. Apart from the label and the feature vector, each record contains a few metadata entries, like the timestamp of the record, that are used further in the evaluation process.
  \newpage
  \section{Evaluation measures}
    There is a plenty of measures that can be observed in classifier's performance. The most commonly observed is perhaps the {\it classification error}. The problem with classification error is that it does not say much when the dataset is imbalanced, as is the situation with the network data. For example if we had a dataset where there are 10,000 samples and only 1 of them belonged to a positive class, the classifier could classify all samples as negative and it would achieve a classification error of 0.01 \%. In this project we are interested in two different measures, {\it precision} and {\it recall}. In order to compute them, we also need the {\it confusion matrix}.
    \subsection{Confusion matrix}
      In binary classification, we can label the two classes as positive and negative. Confusion matrix divides the classification results into the following categories:
      \begin{description}
      \item [TP:] True positives. The number of positive observations the classifier labeled as positive.
      \item [FP:] False positives. The number of negative observations the classifier labeled as positive.
      \item [TN:] True negatives. The number of negative observations the classifier labeled as negative.
      \item [FN:] False negatives. The number of positive observations the classifier labeled as negative.
      \end{description}
      These categories are then used to compute other, more interesting measures.
      \\~\\
      In multiclass classification, the confusion matrix gets more complicated. Consider a dataset with 3 different classes A, B and C. If the classifier takes a sample that belongs to class A and labels it as B, then it counts as FN for class A, but as FP for class B. With the network datasets it gets even more confusing. The samples are either negative and thus belonging to one specific class, or positive, which means one of multiple positive classes. For that reason, samples belonging to the negative classes that are classified as negative do not count as TP. Similarly, when a positive sample is labeled as positive but a different class, it should not be counted as FN, rather as TPish.
    \newpage
    \subsection{Precision}
      {\it Precision}, or {\it positive predictive value} is defined as:
      \begin{displaymath}
        Precision = \frac{TP}{TP + FP}
      \end{displaymath}
      Precision is the fraction of positive objects among all objects that the classifier labeled as positive. It can also be interpreted as the probability that a positively labeled object is truly positive. A precision score of 1.0 means that every object labeled as positive by the classifier is truly positive. It does not say, however, anything about the classifier's ability to recognize all truly positive instances. In multiclass environment, precision of every class varies because it is computed separately.
    \subsection{Recall}
      {\it Recall}, or {\it sensitivity} is defined as:
      \begin{displaymath}
        Recall = \frac{TP}{TP + FN}
      \end{displaymath}
      Recall is the fraction of positive objects that the classifier labeled as positive among all truly positive objects. It can be interpreted as the probability that a truly positive object is labeled as positive. A recall score of 1.0 means that every truly positive object is labeled as positive by the classifier. This could be achieved simply by labeling all objects as positive, recall does not say anything about the number of false positives. Recall of every class varies among all classes in multiclass environment.
  \newpage
  \section{Toolkit implementation}
      This chapter takes a long, hard look at the toolkit's implementation process and the three modules it consists of. The programming language chosen for this task is Python because of its advantages and convenience when it comes to fast prototyping and machine learning.
    \subsection{Loading tool}
    \newpage
    \subsection{Classification tool}
    \newpage
    \subsection{Evaluation tool}
    \newpage
  \section{Future work}
    \subsection{Random forests}
    \newpage
  \section*{Conclusion}
  \addcontentsline{toc}{section}{Conclusion}
\end{document}
