\documentclass[11pt]{article}

\usepackage{mathtools} 
\usepackage[utf8]{inputenc}
\usepackage[czech]{babel}
\usepackage[]{algorithm2e}
\usepackage{url}

\urlstyle{same}

\addto\captionsczech{
  \renewcommand{\contentsname}
    {Table of contents}
}

\title{Handling Missing Values in Decision Forests in the Encrypted Network Traffic}
\date{2018-04-24}
\author{Lukáš Sahula}

\begin{document}
  \maketitle
  \newpage
  \section*{Acknowledgement}
    TODO: Acknowledgement
  \newpage
  \section*{Abstract}
    TODO: Abstract
    \\~\\
    {\bf Keywords:} malware, classification, random forests, supervised learning, missing values, imputation 
    \\~\\
    TODO: Abstrakt cz
    \\~\\
    {\bf Klíčová slova:} malware, klasifikace, náhodné lesy, učení s učitelem, chybějící hodnoty, imputace 
  \newpage
  \tableofcontents
  \newpage

  \section*{Introduction}
  \addcontentsline{toc}{section}{Introduction}
    TODO: Introduction
    \subsection{Why}
      TODO: Why
    \subsection{Summary}
      TODO: Summary
    \subsection{Intro to classification}
      TODO: Intro to classification
  \newpage
  \section{Random forests}
    TODO: Random forests
  \newpage
  \section{Handling missing values}
    In statistics, {\it missing values} or {\it missing data} mark the absence of value in a feature variable of an observation. Missing data in a dataset make it difficult for machine learning algorithms to properly learn from it. Depending on their amount and type they can hinder the classifiers performance and even, in some cases, render their output completely worthless.
    \\~\\
    There are various ways of handling these missing values, some as simple as dropping the the compromised observations altogether, leaving only those that are fully present. This however, cannot be done when there are missing data somewhere in most of the observations. Other simple methods include replacing missing data with the mean or median of all the non-missing data of the feature in question, or replacing the missing data with a constant outside of the interval of that feature's value. The process of replacing the missing data is called {\it imputation}.
    \\~\\
    More sophisticated methods of imputation also exist, a few of which are discussed more thoroughly in this thesis. Some methods work better with smaller datasets, or with datasets with low missingness ratio. The amount of correlation between feature variables can also be important to some methods and not that important to others. Some imputation algorithms are very slow and using them in experiments with big datasets can get seriously time-consuming. In other words, choosing a relevant method of imputation for a given dataset is not a simple task and it can prove useful to do an analysis of the data as well as an analysis of the imputation algorithm itself beforehand.
    \subsection{Related work}
      TODO: Related work
    \subsection{Types of missingness}
      Missing data are usually divided into three categories, depending on the characteristics of their missingness. These categories are also called missing data mechanisms. If the missing data are a random subset of all observations, they will have a similar distribution. There is no relationship between whether a value is missing and any other values in the data set, missing or observed. These values are {\it missing completely at random} (MCAR). When data are MCAR, an analysis of it is unbiased. However, data that are truly MCAR are not encountered often.
      \\~\\
      When the missing data is somehow dependent or related to other non-missing values in the observation, the data is {\it missing at random} (MAR). There is no definitive way of distinguishing between MCAR and MAR data. The assumption of it being one or the other is only as good as the knowledge of the data and the field of the one proposing the assumption. A good example of MAR data would be that males are less likely to fill a depression survey, although it does not have any connection to their level of depression.
      \\~\\
      The last missing data mechanism is {\it missing not at random} (MNAR). It means that there is a relationship between value of the variable that is missing and the reason why it is missing in the first place. For example a male suffering from a strong depression can decide not to fill a depression survey because of said depression.
    \subsection{Selected algorithms}
      TODO: Selected algorithms
      \subsubsection{On the fly imputation}
        TODO: On the fly imputation
      \subsubsection{Missingness incorporated in attributes}
        TODO: Missingness incorporated in attributes
  \newpage
  \section{Network dataset}
    TODO: Network datasets
    \subsection{Dataset description}
      TODO: Dataset description
    \subsection{Analysis with correlation}
      TODO: Analysis withcorrelation
    \subsection{Why most of the algorithms is not relevant}
      TODO: Why most of the algorithms is not relevant (big data)
  \newpage
  \section{Experiments}
    TODO: Experiments
    \subsection{Evaluation metrics}
      TODO: Evaluation metrics
  \newpage
  \section*{Conclusion}
  \addcontentsline{toc}{section}{Conclusion}
    TODO: Conclusion
  \newpage
  \begin{thebibliography}{9}
    \bibitem{brabec}
      Jan Brabec,
      {\it Decision Forests in the Task of Semi-Supervised Learning},
      Czech Technical University in Prague. Computing and Information Centre, 
      2017-01-29

    \bibitem{breiman}
      Leo Breiman,
      {\it Manual--Setting Up, Using And Understanding Random Forests},
      University of California, Berkeley. Department of Statistics,
      Version 4.0,
      Available at \url{www.stat.berkeley.edu/~breiman/} as of 2018-01-12.
    
    \bibitem{python}
      Python Software Foundation,
      {\it Python Language Reference},
      Version 3.6,
      Available at \url{www.python.org} as of 2018-01-12.
 
    \bibitem{pandas}
      Wes McKinney,
      {\it pandas: a python data analysis library},
      2008,
      Available at \url{www.pandas.pydata.org} as of 2018-01-12.

    \bibitem{numpy}
      Travis Oliphant,
      {\it NumPy},
      2005,
      Available at \url{www.numpy.org} as of 2018-01-12.

    \bibitem{scikit}
      Pedregosa, F. and Varoquaux, G. and Gramfor, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.,
      {\it Scikit-learn: Machine Learning in Python},
      Journal of Machine Learning Research,
      Volume 12,
      2011,
      Available at \url{www.scikit-learn.org} as of 2018-01-12.

    \bibitem{bitbucket}
      Atlassian Corporation Plc,
      {\it Bitbucket},
      Available at \url{www.bitbucket.org} as of 2018-01-12.
       
  \end{thebibliography}

\end{document}

